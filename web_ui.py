import gradio as gr
import sqlite3
import json
import asyncio
from typing import List
import networkx as nx
import matplotlib.pyplot as plt
from crawl_web import init_db, init_db_required, unified_crawler
from rag_cli import retrieve_chunks, generate_answer
import os

def get_db_connection():
    """ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥"""
    return sqlite3.connect('local_docs.db', check_same_thread=False)

def build_knowledge_graph():
    """æ·»åŠ åˆå§‹åŒ–è£…é¥°å™¨"""
    @init_db_required
    def _build_graph():
        conn = get_db_connection()
        cursor = conn.cursor()
        G = nx.DiGraph()
        
        # è·å–æ‰€æœ‰é¡µé¢å¹¶å¤„ç†æ ‡é¢˜
        cursor.execute("SELECT url, metadata, COUNT(*) as chunk_count FROM site_pages GROUP BY url")
        pages = {}
        for url, metadata, chunk_count in cursor.fetchall():
            meta = json.loads(metadata)
            # ä¿®æ”¹æ ‡é¢˜ç”Ÿæˆé€»è¾‘
            if 'url_path' in meta and meta['url_path']:
                # ä½¿ç”¨ URL è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ ‡é¢˜
                title = meta['url_path'].rstrip('/').split('/')[-1]
            else:
                # å¦‚æœæ²¡æœ‰ url_pathï¼Œåˆ™ä½¿ç”¨ URL çš„æœ€åä¸€éƒ¨åˆ†
                title = url.rstrip('/').split('/')[-1]
            
            # å¦‚æœæ ‡é¢˜ä¸ºç©ºï¼Œä½¿ç”¨åŸŸå
            if not title:
                title = url.split('/')[2]  # è·å–åŸŸåéƒ¨åˆ†
            
            # ç¾åŒ–æ ‡é¢˜
            title = title.replace('-', ' ').replace('_', ' ').title()
            # é™åˆ¶æ ‡é¢˜é•¿åº¦ï¼Œç¡®ä¿å¯è¯»æ€§
            title = title[:30] + '...' if len(title) > 30 else title
            
            pages[url] = {
                "title": title,
                "size": chunk_count * 50 + 100,
                "depth": len(meta.get('url_path', '').split('/'))
            }
            G.add_node(url, **pages[url])
        
        # å¦‚æœæ²¡æœ‰æ•°æ®æ—¶æ˜¾ç¤ºç©ºå›¾
        if len(pages) == 0:
            plt.figure(figsize=(8, 4))
            plt.text(0.5, 0.5, 'No data available', ha='center', va='center')
            plt.axis('off')
            conn.close()
            return plt.gcf()
        
        # æ·»åŠ å…³è”å…³ç³»
        cursor.execute("SELECT url, content FROM site_pages")
        content_map = {url: content for url, content in cursor.fetchall()}
        
        for url in pages:
            if url in content_map and "http" in content_map[url]:
                links = [link for link in pages.keys() if link in content_map[url]]
                for link in links:
                    G.add_edge(url, link)
        
        plt.figure(figsize=(12, 8))
        pos = nx.kamada_kawai_layout(G)  # ä½¿ç”¨æ›´æ¸…æ™°çš„å¸ƒå±€ç®—æ³•
        
        # æ ¹æ®è·¯å¾„æ·±åº¦è®¾ç½®é¢œè‰²
        colors = [node[1]['depth'] for node in G.nodes(data=True)]
        # æ ¹æ®åˆ†å—æ•°é‡è®¾ç½®å¤§å°
        sizes = [node[1]['size'] for node in G.nodes(data=True)]
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(
            G, pos,
            node_color=colors,
            cmap=plt.cm.viridis,
            node_size=sizes,
            alpha=0.9,
            edgecolors='grey'
        )
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(
            G, pos,
            width=1,
            alpha=0.3,
            edge_color='gray',
            arrowsize=10
        )
        
        # ä¿®æ”¹æ ‡ç­¾ç»˜åˆ¶ï¼Œç¡®ä¿æ ‡ç­¾å¯è§
        labels = {n[0]: n[1]['title'] for n in G.nodes(data=True)}
        print("labels: ",labels)
        nx.draw_networkx_labels(
            G, pos,
            labels=labels,
            font_size=8,  # å‡å°å­—ä½“ä»¥é¿å…é‡å 
            font_color='black',
            font_weight='bold',
            bbox=dict(
                facecolor='white',
                edgecolor='none',
                alpha=0.7,
                pad=0.5
            )
        )
        
        # æ·»åŠ é¢œè‰²æ¡å’Œå›¾ä¾‹ï¼ˆæ·»åŠ æœ‰æ•ˆæ€§æ£€æŸ¥ï¼‰
        if colors and len(set(colors)) > 1:  # åªåœ¨é¢œè‰²æ•°æ®æœ‰æ•ˆæ—¶æ·»åŠ 
            sm = plt.cm.ScalarMappable(
                cmap=plt.cm.viridis,
                norm=plt.Normalize(vmin=min(colors), vmax=max(colors))
            )
            sm.set_array([])
            cbar = plt.colorbar(sm, ax=plt.gca(), shrink=0.8)  # æ˜¾å¼æŒ‡å®šaxå‚æ•°
            cbar.set_label('URL Path Depth')
        elif colors:  # æ‰€æœ‰èŠ‚ç‚¹é¢œè‰²ç›¸åŒçš„æƒ…å†µ
            sm = plt.cm.ScalarMappable(
                cmap=plt.cm.viridis,
                norm=plt.Normalize(vmin=0, vmax=1)  # è®¾ç½®é»˜è®¤èŒƒå›´
            )
            sm.set_array([])
            cbar = plt.colorbar(sm, ax=plt.gca(), shrink=0.8)
            cbar.set_label('URL Path Depth (All Same)')
        
        plt.title("Knowledge Graph", fontsize=16)
        plt.axis('off')
        # nx.draw(G, pos, with_labels=True)
        conn.close()
        return plt.gcf()
    return _build_graph()

async def rag_pipeline(question: str):
    """ä¿®æ”¹åçš„RAGæµç¨‹"""
    conn = get_db_connection()
    cursor = conn.cursor()
    from rag_cli import get_embedding  # å»¶è¿Ÿå¯¼å…¥é¿å…å†²çª
    
    question_embedding = await get_embedding(question)
    chunks = retrieve_chunks(question_embedding)
    
    if not chunks:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯", ""
    
    context = "\n\n".join([f"[æ¥æºï¼š{c['metadata']['url_path']}]\n{c['content']}" for c in chunks])
    answer = await generate_answer(question, context)
    sources = "\n".join([f"- {c['source']}" for c in chunks])
    conn.close()
    return answer, sources

def get_db_data():
    """çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“æŸ¥è¯¢"""
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    cursor.execute("SELECT url, COUNT(*) FROM site_pages GROUP BY url")
    data = cursor.fetchall()
    conn.close()
    return data

def get_db_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    stats = {
        "æ€»é¡µé¢æ•°": cursor.execute("SELECT COUNT(DISTINCT url) FROM site_pages").fetchone()[0],
        "æ€»åˆ†å—æ•°": cursor.execute("SELECT COUNT(*) FROM site_pages").fetchone()[0],
        "å­˜å‚¨å¤§å°": f"{os.path.getsize('local_docs.db')/1024/1024:.2f} MB"
    }
    conn.close()
    return stats

def build_ui():
    """æ„å»ºå®Œæ•´çš„å‰ç«¯ç•Œé¢"""
    with gr.Blocks(title="çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ§  æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ")
        
        with gr.Tab("ğŸŒ ç½‘é¡µæŠ“å–"):
            with gr.Row():
                url_input = gr.Textbox(label="è¾“å…¥ç½‘å€", placeholder="https://example.com")
            with gr.Row():
                max_pages_input = gr.Number(label="æœ€å¤§æŠ“å–é¡µæ•°", value=100, precision=0)
                time_limit_input = gr.Number(label="æ—¶é—´é™åˆ¶(ç§’)", value=300, precision=0)
            with gr.Row():
                crawl_btn = gr.Button("å¼€å§‹æŠ“å–", variant="primary")
                # stop_btn = gr.Button("åœæ­¢æŠ“å–")
            progress = gr.Slider(visible=True, label="æŠ“å–è¿›åº¦", interactive=False)
            gr.Markdown("### å®æ—¶ç»Ÿè®¡")
            stats_panel = gr.JSON(label="æŠ“å–ç»Ÿè®¡", value={
                "å·²æŠ“å–é¡µé¢": 0,
                "å‰©ä½™é¡µé¢": 0,
                "é¢„è®¡å‰©ä½™æ—¶é—´": "N/A"
            }, every=5)
            log_output = gr.Textbox(label="æ“ä½œæ—¥å¿—", interactive=False)
        
        with gr.Tab("â“ é—®ç­”"):
            with gr.Row():
                question_input = gr.Textbox(label="è¾“å…¥é—®é¢˜", lines=3)
                answer_output = gr.Markdown(label="ç­”æ¡ˆ")
            with gr.Accordion("æŸ¥çœ‹å‚è€ƒæ¥æº", open=False):
                sources_output = gr.Markdown()
            ask_btn = gr.Button("æäº¤é—®é¢˜", variant="primary")
        
        with gr.Tab("ğŸ“Š çŸ¥è¯†å›¾è°±"):
            plot = gr.Plot(label="çŸ¥è¯†å…³è”å›¾è°±", every=60)
            gr.Button("åˆ·æ–°å›¾è°±").click(build_knowledge_graph, outputs=plot)
        
        with gr.Tab("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                stats = gr.JSON(label="ç»Ÿè®¡ä¿¡æ¯")
                with gr.Column():
                    db_view = gr.Dataframe(
                        headers=["URL", "åˆ†å—æ•°"],
                        datatype=["str", "number"],
                        interactive=False
                    )
                    gr.Button("åˆ·æ–°æ•°æ®").click(
                        get_db_data,  # ç›´æ¥ä½¿ç”¨å°è£…å¥½çš„å‡½æ•°
                        outputs=db_view
                    )
            gr.Button("æ›´æ–°ç»Ÿè®¡").click(
                get_db_stats,  # ä½¿ç”¨å°è£…å¥½çš„ç»Ÿè®¡å‡½æ•°
                outputs=stats
            )

        # äº‹ä»¶å¤„ç†
        crawl_btn.click(
            fn=lambda url, max_p, time_l: asyncio.run(unified_crawler(base_url=url, max_pages=max_p, time_limit=time_l)),
            inputs=[url_input, max_pages_input, time_limit_input],
            outputs=[stats_panel, log_output]
        )
        
        
        ask_btn.click(
            rag_pipeline,
            inputs=question_input,
            outputs=[answer_output, sources_output]
        )
        
        # stop_btn.click(
        #     fn=lambda: setattr(crawler, 'should_stop', True),
        #     outputs=None
        # )
        
        demo.load(build_knowledge_graph, outputs=plot)

    return demo

if __name__ == "__main__":
    init_db()  # æ˜¾å¼åˆå§‹åŒ–
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
    build_knowledge_graph()
    
    # å¯åŠ¨å‰ç«¯
    web_ui = build_ui()
    web_ui.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )

async def start_crawling(url: str, max_pages: int, time_limit: int):
    from crawl_web import unified_crawler
    try:
        await unified_crawler(
            base_url=url,
            max_depth=3,
            max_concurrent=8,
            max_pages=max_pages,
            time_limit=time_limit
        )
        return get_db_stats(), "æŠ“å–å®Œæˆï¼"
    except Exception as e:
        return get_db_stats(), f"æŠ“å–ä¸­æ–­: {str(e)}"

def get_crawl_stats():
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    stats = {
        "å·²æŠ“å–é¡µé¢": cursor.execute("SELECT COUNT(DISTINCT url) FROM site_pages").fetchone()[0],
        "å‰©ä½™é¡µé¢": "N/A",  # éœ€è¦crawleræä¾›å®æ—¶æ•°æ®
        "é¢„è®¡å‰©ä½™æ—¶é—´": "N/A"  # éœ€è¦è®¡ç®—é€»è¾‘
    }
    conn.close()
    return stats 