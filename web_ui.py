import gradio as gr
import sqlite3
import json
import asyncio
from typing import List
import networkx as nx
import matplotlib.pyplot as plt
from crawl_web import init_db, init_db_required, unified_crawler
from rag_cli import retrieve_chunks, generate_answer
import os

def get_db_connection():
    """ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥"""
    return sqlite3.connect('local_docs.db', check_same_thread=False)

def build_knowledge_graph():
    """æ·»åŠ åˆå§‹åŒ–è£…é¥°å™¨"""
    @init_db_required  # æ–°å¢è£…é¥°å™¨
    def _build_graph():
        conn = get_db_connection()
        cursor = conn.cursor()
        G = nx.DiGraph()
        
        # è·å–æ‰€æœ‰é¡µé¢
        cursor.execute("SELECT url, metadata FROM site_pages")
        pages = {}
        for url, metadata in cursor.fetchall():
            meta = json.loads(metadata)
            if url not in pages:
                G.add_node(url, 
                          title=meta.get('url_path', ''),
                          size=meta.get('chunk_size', 0))
                pages[url] = []
            pages[url].append(meta)
        
        # æ·»åŠ å…³è”å…³ç³»
        for url in pages:
            cursor.execute("SELECT content FROM site_pages WHERE url=?", (url,))
            content = " ".join([row[0] for row in cursor.fetchall()])
            if "http" in content:
                links = [link for link in pages.keys() if link in content]
                for link in links:
                    G.add_edge(url, link)
        
        plt.figure(figsize=(12, 8))
        pos = nx.spring_layout(G)
        nx.draw(G, pos, with_labels=True, node_size=200, font_size=8)
        conn.close()  # æ·»åŠ å…³é—­è¿æ¥
        return plt.gcf()
    return _build_graph()

async def rag_pipeline(question: str):
    """ä¿®æ”¹åçš„RAGæµç¨‹"""
    conn = get_db_connection()
    cursor = conn.cursor()
    from rag_cli import get_embedding  # å»¶è¿Ÿå¯¼å…¥é¿å…å†²çª
    
    question_embedding = await get_embedding(question)
    chunks = retrieve_chunks(question_embedding)
    
    if not chunks:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯", ""
    
    context = "\n\n".join([f"[æ¥æºï¼š{c['metadata']['url_path']}]\n{c['content']}" for c in chunks])
    answer = await generate_answer(question, context)
    sources = "\n".join([f"- {c['source']}" for c in chunks])
    conn.close()
    return answer, sources

def get_db_data():
    """çº¿ç¨‹å®‰å…¨çš„æ•°æ®åº“æŸ¥è¯¢"""
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    cursor.execute("SELECT url, COUNT(*) FROM site_pages GROUP BY url")
    data = cursor.fetchall()
    conn.close()
    return data

def get_db_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    stats = {
        "æ€»é¡µé¢æ•°": cursor.execute("SELECT COUNT(DISTINCT url) FROM site_pages").fetchone()[0],
        "æ€»åˆ†å—æ•°": cursor.execute("SELECT COUNT(*) FROM site_pages").fetchone()[0],
        "å­˜å‚¨å¤§å°": f"{os.path.getsize('local_docs.db')/1024/1024:.2f} MB"
    }
    conn.close()
    return stats

def build_ui():
    """æ„å»ºå®Œæ•´çš„å‰ç«¯ç•Œé¢"""
    with gr.Blocks(title="çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ§  æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ")
        
        with gr.Tab("ğŸŒ ç½‘é¡µæŠ“å–"):
            with gr.Row():
                url_input = gr.Textbox(label="è¾“å…¥ç½‘å€", placeholder="https://example.com")
            with gr.Row():
                max_pages_input = gr.Number(label="æœ€å¤§æŠ“å–é¡µæ•°", value=100, precision=0)
                time_limit_input = gr.Number(label="æ—¶é—´é™åˆ¶(ç§’)", value=300, precision=0)
            with gr.Row():
                crawl_btn = gr.Button("å¼€å§‹æŠ“å–", variant="primary")
                # stop_btn = gr.Button("åœæ­¢æŠ“å–")
            progress = gr.Slider(visible=True, label="æŠ“å–è¿›åº¦", interactive=False)
            gr.Markdown("### å®æ—¶ç»Ÿè®¡")
            stats_panel = gr.JSON(label="æŠ“å–ç»Ÿè®¡", value={
                "å·²æŠ“å–é¡µé¢": 0,
                "å‰©ä½™é¡µé¢": 0,
                "é¢„è®¡å‰©ä½™æ—¶é—´": "N/A"
            }, every=5)
            log_output = gr.Textbox(label="æ“ä½œæ—¥å¿—", interactive=False)
        
        with gr.Tab("â“ é—®ç­”"):
            with gr.Row():
                question_input = gr.Textbox(label="è¾“å…¥é—®é¢˜", lines=3)
                answer_output = gr.Markdown(label="ç­”æ¡ˆ")
            with gr.Accordion("æŸ¥çœ‹å‚è€ƒæ¥æº", open=False):
                sources_output = gr.Markdown()
            ask_btn = gr.Button("æäº¤é—®é¢˜", variant="primary")
        
        with gr.Tab("ğŸ“Š çŸ¥è¯†å›¾è°±"):
            plot = gr.Plot(label="çŸ¥è¯†å…³è”å›¾è°±", every=60)
            gr.Button("åˆ·æ–°å›¾è°±").click(build_knowledge_graph, outputs=plot)
        
        with gr.Tab("ğŸ“‚ çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                stats = gr.JSON(label="ç»Ÿè®¡ä¿¡æ¯")
                with gr.Column():
                    db_view = gr.Dataframe(
                        headers=["URL", "åˆ†å—æ•°"],
                        datatype=["str", "number"],
                        interactive=False
                    )
                    gr.Button("åˆ·æ–°æ•°æ®").click(
                        get_db_data,  # ç›´æ¥ä½¿ç”¨å°è£…å¥½çš„å‡½æ•°
                        outputs=db_view
                    )
            gr.Button("æ›´æ–°ç»Ÿè®¡").click(
                get_db_stats,  # ä½¿ç”¨å°è£…å¥½çš„ç»Ÿè®¡å‡½æ•°
                outputs=stats
            )

        # äº‹ä»¶å¤„ç†
        crawl_btn.click(
            fn=lambda url, max_p, time_l: asyncio.run(unified_crawler(base_url=url, max_pages=max_p, time_limit=time_l)),
            inputs=[url_input, max_pages_input, time_limit_input],
            outputs=[stats_panel, log_output]
        )
        
        
        ask_btn.click(
            rag_pipeline,
            inputs=question_input,
            outputs=[answer_output, sources_output]
        )
        
        # stop_btn.click(
        #     fn=lambda: setattr(crawler, 'should_stop', True),
        #     outputs=None
        # )
        
        demo.load(build_knowledge_graph, outputs=plot)

    return demo

if __name__ == "__main__":
    init_db()  # æ˜¾å¼åˆå§‹åŒ–
    # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
    build_knowledge_graph()
    
    # å¯åŠ¨å‰ç«¯
    web_ui = build_ui()
    web_ui.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )

async def start_crawling(url: str, max_pages: int, time_limit: int):
    from crawl_web import unified_crawler
    try:
        await unified_crawler(
            base_url=url,
            max_depth=3,
            max_concurrent=8,
            max_pages=max_pages,
            time_limit=time_limit
        )
        return get_db_stats(), "æŠ“å–å®Œæˆï¼"
    except Exception as e:
        return get_db_stats(), f"æŠ“å–ä¸­æ–­: {str(e)}"

def get_crawl_stats():
    conn = sqlite3.connect('local_docs.db')
    cursor = conn.cursor()
    stats = {
        "å·²æŠ“å–é¡µé¢": cursor.execute("SELECT COUNT(DISTINCT url) FROM site_pages").fetchone()[0],
        "å‰©ä½™é¡µé¢": "N/A",  # éœ€è¦crawleræä¾›å®æ—¶æ•°æ®
        "é¢„è®¡å‰©ä½™æ—¶é—´": "N/A"  # éœ€è¦è®¡ç®—é€»è¾‘
    }
    conn.close()
    return stats 